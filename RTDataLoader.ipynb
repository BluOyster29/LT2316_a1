{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  argparse, numpy as np, pandas as pd, subprocess\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "import csv\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data import Field, Iterator, TabularDataset, BucketIterator\n",
    "import tqdm\n",
    "from tqdm import tnrange\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm_notebook\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_languages(csv_file, preset):\n",
    "    with open(csv_file, 'r') as csv_File: #opens csv containing language codes and their names\n",
    "        reader = csv.reader(csv_File)\n",
    "        language_table = {row[0].split(';')[1] : row[0].split(';')[0] for row in reader} #dictionary mapping code to name\n",
    "    if preset == 'y':\n",
    "        #Preset language codes to be used in model, chosen at random\n",
    "        language_codes = [\"srd\", \"krc\", \"nob\", \"pnb\",\n",
    "                          \"mai\", \"eng\", \"be-tarask\",\n",
    "                          \"xho\", \"tet\", \"tha\"]\n",
    "        language_names = [(key, value) for key, value in language_table.items() if value in language_codes]\n",
    "        return language_names\n",
    "    elif preset == 'n':\n",
    "        '''\n",
    "        experimental function for allowing user to choose which languages to use\n",
    "        '''\n",
    "        languages = []\n",
    "        while len(languages) != 10:\n",
    "            language = input(\"Enter language \").capitalize()\n",
    "            #print(language)\n",
    "            if language in languages:\n",
    "                print(\"You've already said that one! \")\n",
    "            elif language in language_table:\n",
    "                languages.append(language)\n",
    "                print(languages)\n",
    "            else:\n",
    "                print('Language not recognised. Please refer to language labels')\n",
    "                print(languages)\n",
    "                continue\n",
    "        language_codes = [language_table[i] for i in language_table if i in languages] #collects languages from predetermined set,\n",
    "        language_names =  [(key, value) for key, value in language_table.items() if value in language_codes] #dictionary mapping code to name\n",
    "        return language_names\n",
    "    else:\n",
    "        print(\"has to be y or n dummy\") #in case of user error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def gen_data(training_file, training_labels, language_codes, training):\n",
    "    '''\n",
    "    Function generates the set based on pre defined language codes and creates various\n",
    "    attributes to the object\n",
    "    '''\n",
    "    if training == True:\n",
    "        data = [i.split('\\n')[:-1] for i in open(training_file, 'r')] #opens text file and splits on new line\n",
    "        labels = [i.split('\\n')[:-1] for i in open(training_labels, 'r')] #opens label file and splits on white space\n",
    "        things = list(zip([i[0] for i in data], [i[0] for i in labels])) #zips sentences with corrosponding language label\n",
    "        sets = [(i[0],i[1]) for i in things] #this might actually do the same thing as the above not sure\n",
    "        \n",
    "        x = [i[0][:100] for i in sets if i[1] in language_codes] #Matrix of sentences to be used in the model\n",
    "        y = [i[1] for i in sets if i[1] in language_codes] #labels for each of the sentences\n",
    "        raw_data = ''.join([i for i in x]) #concatenation of all characters in the training set\n",
    "        vocab = {char: ord(char) for char in set(raw_data)} #dictionary mapping character to ord(integer)\n",
    "        int2char = {num : char for char, num in vocab.items()} #dictionary mapping integer to character\n",
    "        return x, y, vocab, int2char\n",
    "    else:\n",
    "        data = [i.split('\\n')[:-1] for i in open(training_file, 'r')] #opens text file and splits on new line\n",
    "        labels = [i.split('\\n')[:-1] for i in open(training_labels, 'r')] #opens label file and splits on white space\n",
    "        things = list(zip([i[0] for i in data], [i[0] for i in labels])) #zips sentences with corrosponding language label\n",
    "        sets = [(i[0],i[1]) for i in things] #this might actually do the same thing as the above not sure\n",
    "        x = [i[0][:100] for i in sets if i[1] in language_codes] #Matrix of sentences to be used in the model\n",
    "        y = [i[1] for i in sets if i[1] in language_codes] #labels for each of the sentences\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_names = get_languages('./data/raw/labels.csv', 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_codes = [i[1] for i in language_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = 'data/raw/x_train.txt'\n",
    "y_train = 'data/raw/y_train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, vocab, int2char = gen_data(x_train, y_train, language_codes, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = 'data/raw/x_test.txt'\n",
    "y_test = 'data/raw/y_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = gen_data(x_test, y_test, language_codes, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langencoder(language_codes):\n",
    "    one_hot_lang = {}\n",
    "    lang2int = {lang : (num) for num, lang in dict(enumerate(language_codes)).items()}\n",
    "    '''for lang, num in lang2int.items():\n",
    "        one_hot_lang[lang] = np.zeros(9)\n",
    "        one_hot_lang[lang] = np.insert(one_hot_lang[lang],num, 1)\n",
    "        #one_hot_lang[lang] = list([0,0,0,0,0,0,0,0,0,0,0]).insert(num, 1)\n",
    "    '''\n",
    "    #hot2lang = {hot : lang for lang, hot in one_hot_lang.items()}\n",
    "    \n",
    "    return lang2int#, hot2lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2int = langencoder(language_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(x_train):\n",
    "    total_data = ''.join(x_train)\n",
    "    int2char = dict(enumerate(set(total_data)))\n",
    "    char2int = {char : (num + 1) for num, char in int2char.items() }\n",
    "    char2int['<niv>'] = max(char2int.values()) +1\n",
    "    return char2int\n",
    "\n",
    "vocab = build_vocab(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vocab(vocab):\n",
    "    one_hot_vocab = {}\n",
    "    for char, num in vocab.items():\n",
    "        empty_vector = np.zeros(len(vocab) + 1)\n",
    "        one_hot_vocab[char] = np.insert(empty_vector, num, 1)\n",
    "    one_hot_vocab['<niv>'] = np.insert(np.zeros(len(vocab) + 1), len(vocab) + 1, 1)\n",
    "    return one_hot_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_vocab = one_hot_vocab(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(x,y, lang2int,vocab):\n",
    "    labels = []\n",
    "    vectors = []\n",
    "    sets = zip(x,y)\n",
    "    for samples in sets:\n",
    "        \n",
    "        sample = [i for i in samples[0]]\n",
    "        label = samples[1]\n",
    "        #print(label)\n",
    "        count = 100\n",
    "        while count != 0:\n",
    "            #x = [(i,int2label[samples[1]]) for i in samples[0]]\n",
    "            vector = []\n",
    "            encoded = []\n",
    "            for i in sample:\n",
    "                if i in vocab:\n",
    "                    encoded.append(vocab[i])\n",
    "                else:\n",
    "                    encoded.append(vocab['<niv>'])\n",
    "                \n",
    "            for i in range(1,101):\n",
    "                vectors.append(torch.LongTensor(encoded[:i])) #, int(lang2int[label])))\n",
    "                labels.append(lang2int[label])\n",
    "                count -=1\n",
    "            #vectors += vector\n",
    "            #label_matrix += labels\n",
    "    return pad_sequence(vectors, batch_first=True, padding_value=0), labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = build_data(x_train, y_train, lang2int, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_labels = build_data(x_test, y_test, lang2int, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTDataset(Dataset):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.train_x[index], self.train_y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_x)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = RTDataset(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set = RTDataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data_set, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data_set, batch_size=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, input_size, hidden_size, num_layers, output_size, dev, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len = seq_len\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dev = dev\n",
    "        self.emb = nn.Embedding(vocab_size, input_size)\n",
    "        self.gru = nn.GRU(input_size, hidden_size,\n",
    "                          num_layers=self.num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * seq_len, output_size)\n",
    "        # self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, sequence, hidden_layer):\n",
    "        output = self.emb(sequence)\n",
    "        hidden_layer = hidden_layer.to(self.dev)\n",
    "        output, hidden_layer = self.gru(output, hidden_layer)\n",
    "        output = output.contiguous().view(-1, self.hidden_size *\n",
    "                                          len(sequence[0]))\n",
    "        output = self.fc(output)\n",
    "        # don't need the softmax here as CrossEntropy loss already does softmax at its end\n",
    "        # output = self.softmax(output)\n",
    "        return output, hidden_layer\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, os, zipfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'addpermissions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-6b46ae18b34e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddpermissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'addpermissions'"
     ]
    }
   ],
   "source": [
    "import os, base64, addpermissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders= [train_loader,test_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataloaders(filepath):\n",
    "    directory = 'dataloaders/'\n",
    "    dataloaders = []\n",
    "    for filename in os.listdir(filepath):\n",
    "        file = gzip.GzipFile(directory + filename, 'rb')\n",
    "        pickle.load(file.read())\n",
    "      \n",
    "\n",
    "    return dataloaders[0], dataloaders[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "file must have 'read' and 'readline' attributes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-fe0942ca8f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataloaders/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-131-e911eaf0de60>\u001b[0m in \u001b[0;36mload_dataloaders\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have 'read' and 'readline' attributes"
     ]
    }
   ],
   "source": [
    "training, testing = load_dataloaders('dataloaders/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(train_loader, 'train_loader.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'dataloaders/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling Dataloader dataloaders/training_dataloader.zip\n",
      "Pickling Dataloader dataloaders/testing_dataloader.zip\n"
     ]
    }
   ],
   "source": [
    "training = 'dataloaders/training_dataloader.zip'\n",
    "testing = 'dataloaders/testing_dataloader.zip'\n",
    "for i in zip([training,testing], loaders):\n",
    "    print('Pickling Dataloader {}'.format(str(i[0])))\n",
    "    file = gzip.GzipFile(i[0], 'wb')\n",
    "    file.write(pickle.dumps(i[1], 1))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping\n"
     ]
    }
   ],
   "source": [
    "training = 'dataloaders/training_dataloader.pickle'\n",
    "testing = 'dataloaders/testing_dataloader.pickle'\n",
    "for i in zip([training,testing], loaders):\n",
    "    print('Pickling Dataloader {}'.format(str(i[0])))\n",
    "    with open(i[0], 'wb') as output_file:\n",
    "        pickle.dump(i[1], output_file)\n",
    "        output_file.close()\n",
    "print('Zipping')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-1a2493ed2df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_dataloader.pickle.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.7.4_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile('training_dataloader.pickle.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "seq_len = 100\n",
    "batch_size = 200\n",
    "input_size = 100\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "output_size = 10\n",
    "dev = torch.device('cuda:01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUNet(vocab_size, seq_len, input_size, \n",
    "               hidden_size, num_layers, output_size, dev, dropout=0.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "model = model.to(dev)\n",
    "print('Training')\n",
    "for i in range(5):\n",
    "    \n",
    "    epoch_loss = []\n",
    "    h = model.init_hidden(batch_size)\n",
    "    count = 0\n",
    "    \n",
    "    percent = 10\n",
    "    for (x, y) in train_loader:\n",
    "        tenp = round(len(x) / 0.2)\n",
    "        count +=1 \n",
    "        x = x.to(dev)\n",
    "        y = y.to(dev)\n",
    "        optimizer.zero_grad()\n",
    "        h = h.data\n",
    "        out, h = model(x, h)\n",
    "        loss = criterion(out, y.long())\n",
    "        loss.backward()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        if count % tenp == 0:\n",
    "            print('Training Epoch {}% complete'.format(percent))\n",
    "            percent += 10\n",
    "        #print('Loss per timestep = {}'.format(loss.item()))\n",
    "        \n",
    "        \n",
    "    avg_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "    print(\"Average loss at epoch %d: %.7f\" % (i + 1, avg_loss)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_model_try = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    hidden_layer = model.init_hidden(1)\n",
    "    for examples in batch[0]:\n",
    "        prediction = gru_model_try(examples.unsqueeze(0).to('cuda:01'), hidden_layer)\n",
    "        \n",
    "        break\n",
    "    for labels in batch[1]:\n",
    "        #print(labels, hidden_layer)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = 'dataloaders/training_dataloader.pickle'\n",
    "testing = 'dataloaders/testing_dataloader.pickle'\n",
    "for i in zip([training,testing], loaders):\n",
    "    print('Pickling Dataloader {}'.format(str(i[0])))\n",
    "    with open(i[0], 'wb') as output_file:\n",
    "        pickle.dump(i[1], output_file)\n",
    "        output_file.close()\n",
    "print('Zipping')\n",
    "for i in os.listdir(dir):\n",
    "    output_zipped = zipfile.ZipFile(i, 'w')\n",
    "    output_zipped.write(dir + i, compress_type=ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
